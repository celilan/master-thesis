{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACIT5900: Master Thesis\n",
    "### *Exploratory Data Analysis*\n",
    "\n",
    ">-------------------------------------------\n",
    "> *Spring 2025*\n",
    "\n",
    ">--------------------------------------------\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "1. [**Basic Statistics**](#statistics)<br>\n",
    "2. [**Visualize Content Column**](#content-visualization)<br>\n",
    "3. [**Visualize Other Columns**](#others-visualization)<br>\n",
    "4. [**Co-authorship Knowledge Graph**](#knowledge-graph)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules needed\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('df_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"statistics\"></a> 1) Basic Statistics\n",
    "\n",
    "[‚¨ÜÔ∏è Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate basic statistics\n",
    "df['text_length'] = df['cleaned_content'].apply(len)  \n",
    "df['text_length'].describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"content-visualization\"></a> 2) Visualize Content Column\n",
    "\n",
    "[‚¨ÜÔ∏è Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of words to remove\n",
    "stop_words = set(ENGLISH_STOP_WORDS)  \n",
    "\n",
    "# tokenize the text and remove stop words, punctuation, and numbers\n",
    "words = ' '.join(df['cleaned_content']).split()\n",
    "filtered_words = [\n",
    "    word for word in words \n",
    "    if word.lower() not in stop_words and word not in string.punctuation and not word.isdigit()\n",
    "]\n",
    "\n",
    "# count word frequencies\n",
    "word_counts = Counter(filtered_words)\n",
    "\n",
    "# top 20 most common words\n",
    "common_words = word_counts.most_common(20)\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)\n",
    "\n",
    "# plot the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english', max_features=20)\n",
    "X = vectorizer.fit_transform(df['cleaned_content'])\n",
    "\n",
    "# extract top 20 bigrams\n",
    "bigram_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(bigram_df.sum(axis=0).sort_values(ascending=False).head(20)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words to exclude from bigram\n",
    "stop_words = ['et al', '10 10', 'doi']\n",
    "\n",
    "# extract bigrams without limiting the number of features\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
    "X = vectorizer.fit_transform(df['cleaned_content'])\n",
    "bigram_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# get all bigrams\n",
    "all_bigrams = bigram_df.sum(axis=0)\n",
    "\n",
    "# filter out the unwanted bigrams\n",
    "filtered_bigrams = all_bigrams[~all_bigrams.index.str.contains('|'.join(stop_words))]\n",
    "\n",
    "# create word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(filtered_bigrams.to_dict())\n",
    "\n",
    "# plot the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"others-visualization\"></a> 3) Visualize Other Columns\n",
    "\n",
    "[‚¨ÜÔ∏è Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of articles extracted for each year\n",
    "df['year_published'].value_counts().sort_index().plot(kind='bar', figsize=(10, 5))\n",
    "plt.title('Distribution of Articles by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"knowledge-graph\"></a> 4) Co-authorship Knowledge Graph\n",
    "\n",
    "[‚¨ÜÔ∏è Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['cleaned_authors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to python list\n",
    "df['cleaned_authors'] = df['cleaned_authors'].apply(lambda x: literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_authors(authors):\n",
    "    \"\"\"\n",
    "    Clean and normalize names to 'Firstname Lastname' format,\n",
    "    splitting up incorrectly merged names if needed.\n",
    "    \"\"\"\n",
    "    normalized = []\n",
    "    for name in authors:\n",
    "        \n",
    "        # split if multiple names got merged into one string\n",
    "        if ',' in name and len(name.split()) > 4:\n",
    "            parts = re.split(r',| and ', name)\n",
    "        else:\n",
    "            parts = [name]\n",
    "        \n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if ',' in part:\n",
    "                last, first = part.split(',', 1)\n",
    "                full_name = f\"{first.strip()} {last.strip()}\"\n",
    "            else:\n",
    "                full_name = part\n",
    "            normalized.append(full_name)\n",
    "    return normalized\n",
    "\n",
    "df['normalized_authors'] = df['cleaned_authors'].apply(normalize_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_variants(authors):\n",
    "    \"\"\"\n",
    "    Merge name variants using a mix of fuzzy initials + last name,\n",
    "    plus manual aliasing for known cases like 'Gustavo Mello'.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    for name in authors:\n",
    "        name_clean = name.lower().strip()\n",
    "        parts = name_clean.split()\n",
    "        \n",
    "        # fuzzy key: first initial + last name\n",
    "        if len(parts) > 1:\n",
    "            fuzzy_key = f\"{parts[0][0]}_{parts[-1]}\"\n",
    "        else:\n",
    "            fuzzy_key = name_clean\n",
    "\n",
    "        # manual fix for Gustavo Mello variants\n",
    "        if (\"gustavo\" in name_clean and \"mello\" in name_clean) or name_clean in [\n",
    "            \"mello\", \"g. mello\", \"g. b. m. mello\", \"gustavo b. m.\",\n",
    "            \"gustavo borges mello\", \"gustavo moreno mello\",\n",
    "            \"gustavo borges moreno e.\", \"gustavo borges moreno e. mello\"\n",
    "        ]:\n",
    "            canonical = \"Gustavo Mello\"\n",
    "\n",
    "        else:\n",
    "            # choose name from longest in fuzzy group \n",
    "            existing = [a for a in authors if a.lower().startswith(parts[0][0]) and a.lower().endswith(parts[-1])]\n",
    "            canonical = max(existing, key=len) if existing else name\n",
    "\n",
    "        merged.append(canonical)\n",
    "    return merged\n",
    "\n",
    "df['merged_authors'] = df['normalized_authors'].apply(merge_variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for authors in df['merged_authors']:\n",
    "    unique_authors = list(set(authors))\n",
    "    edges += combinations(unique_authors, 2)\n",
    "\n",
    "# count co-authorship frequency\n",
    "edge_weights = Counter(edges)\n",
    "\n",
    "# build and visualize the graph \n",
    "G = nx.Graph()\n",
    "for (a1, a2), weight in edge_weights.items():\n",
    "    G.add_edge(a1, a2, weight=weight)\n",
    "\n",
    "# draw the graph\n",
    "plt.figure(figsize=(18, 18))\n",
    "pos = nx.spring_layout(G, k=0.7, seed=42)\n",
    "degrees = dict(G.degree())\n",
    "\n",
    "nx.draw_networkx_nodes(\n",
    "    G, pos,\n",
    "    node_size=[300 + degrees[n] * 100 for n in G.nodes],\n",
    "    node_color='lightblue',\n",
    "    edgecolors='black'\n",
    ")\n",
    "nx.draw_networkx_edges(\n",
    "    G, pos,\n",
    "    width=[G[u][v]['weight'] for u, v in G.edges],\n",
    "    alpha=0.5\n",
    ")\n",
    "nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "\n",
    "plt.title(\"üßπ Final Co-authorship Graph ‚Äî Cleaned & Merged\", fontsize=18)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
